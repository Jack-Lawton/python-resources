{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "## In This Lesson\n",
    "* Tokenization\n",
    "* Vectorization\n",
    "* Topic Modelling\n",
    "* Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP\n",
    "\n",
    "Natural Language Processing, NLP is the field of study relating to how computers can process free text data. NLP is a continuously evolving field, some of the most cutting edge machine learning and artificial intelligence programs are NLP focused. For this lesson, I will cover the basics of simple NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Before we get started, let's download a dataset, for this lesson, I will be using The Complete Works of William Shakespeare from [Project Gutenberg](https://www.gutenberg.org/ebooks/100).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare', 'This eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org. If you are not located in the United States, you\\nwill have to check the laws of the country where you are located before\\nusing this eBook.', 'Title: The Complete Works of William Shakespeare', 'Author: William Shakespeare', 'Release Date: January 1994 [eBook #100]\\n[Most recently updated: April 25, 2021]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import requests to download the data\n",
    "import requests\n",
    "\n",
    "# Download shakespeare\n",
    "response = requests.get(\"https://www.gutenberg.org/files/100/100-0.txt\")\n",
    "raw_text = response.text\n",
    "\n",
    "# Trim first three characters (formatting)\n",
    "trim_text = raw_text[3:]\n",
    "\n",
    "# Split up into individual lines\n",
    "# In this dataset, individual lines are split by two new line characters\n",
    "lines = trim_text.replace(\"\\r\", \"\").split(\"\\n\\n\")\n",
    "\n",
    "# Print the first few lines to check this has worked\n",
    "print(lines[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "In order to 'read' text, the first basic principle for a computer to understand is where a word starts and begins.\n",
    "\n",
    "A string is just a list of characters, there is nothing inherently special about spaces or punctuation.\n",
    "\n",
    "Let's take a look at the first line from our dataset and try split it up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_line = lines[0]\n",
    "print(first_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise\n",
    "Write a function to split up the first line into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Complete', 'Works', 'of', 'William', 'Shakespeare,', 'by', 'William', 'Shakespeare']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_into_words(line):\n",
    "    return line.split(\" \")\n",
    "\n",
    "print(split_into_words(first_line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst this solution does work, it can be improved, really we don't want punctuation characters to appear as part of words. i.e. `'Shakespeare,'` should be `'Shakespeare'`.\n",
    "\n",
    "We can also improve this by enforcing lower case, so it does not matter if a word is at the start of a sentence or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the regular expression library to pick out punctuation caracters\n",
    "import re\n",
    "\n",
    "def split_into_words(line):\n",
    "    # Remove punctuation\n",
    "    line_no_punct = re.sub(\"[^\\w\\d\\s]\", \"\", line)\n",
    "    # Colapse whitespace\n",
    "    line_no_long_space = re.sub(\"\\s+\", \" \", line_no_punct)\n",
    "    # Push to lower case\n",
    "    line_lower = line_no_long_space.lower()\n",
    "    return line_lower.split(\" \")\n",
    "\n",
    "print(split_into_words(first_line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have simply removed all punctuation. While punctuation does carry meaning, for more simple NLP solutions it is typically ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a tokenization function, we can use that to tokenize all of our lines, giving us a list of lists of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare'], ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook'], ['title', 'the', 'complete', 'works', 'of', 'william', 'shakespeare'], ['author', 'william', 'shakespeare'], ['release', 'date', 'january', '1994', 'ebook', '100', 'most', 'recently', 'updated', 'april', '25', '2021']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize all lines\n",
    "tokenized_lines = [split_into_words(line) for line in lines]\n",
    "\n",
    "# Print the first few lines to check this has worked\n",
    "print(tokenized_lines[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Vectorization\n",
    "\n",
    "Even though tokenization has broken down our lines into groups of words, these are still not ideal for us to process in a machine learning algorithm. Computers like numbers. Vectorization is the conversion of these words to numbers, such that each unique word can be identified by its own number.\n",
    "\n",
    "Rather than program this ourselves, let's take advantage of [Scikit-learn](https://scikit-learn.org/), which has [an algorithm for this very purpose](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4843)\t1\n",
      "  (0, 6513)\t1\n",
      "  (0, 10044)\t1\n",
      "  (0, 14023)\t1\n",
      "  (0, 20945)\t2\n",
      "  (0, 23800)\t1\n",
      "  (0, 26939)\t2\n",
      "  (0, 30289)\t2\n",
      "  (0, 34190)\t2\n",
      "  (0, 34514)\t1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import count vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate our vectorizer (it works just like any other sklearn class)\n",
    "# We need to provide our tokenizer function as a parameter\n",
    "vectorizer = CountVectorizer(tokenizer=split_into_words)\n",
    "\n",
    "# Fit vectorizer\n",
    "vectorizer.fit(lines)\n",
    "\n",
    "# Vectorize (transform) our data\n",
    "vectorized_lines = vectorizer.transform(lines)\n",
    "\n",
    "# Print the first line, to check it has worked\n",
    "print(vectorized_lines[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type is now a sparse matrix, we can check that the number of rows matches the number of unique words and the number with the value `2` matches duplicated words.\n",
    "\n",
    "Or even better, map them back to the origional words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by: 1\n",
      "complete: 1\n",
      "ebook: 1\n",
      "gutenberg: 1\n",
      "of: 2\n",
      "project: 1\n",
      "shakespeare: 2\n",
      "the: 2\n",
      "william: 2\n",
      "works: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a word lookup dictionary.\n",
    "# The vocabulary_ dictionary from the vectorizer is the wrong way round\n",
    "word_lookup = {value: key for key, value in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# Loop all rows from the first document and print details\n",
    "for key, frequency in zip(vectorized_lines[0].indices, vectorized_lines[0].data):\n",
    "    print(\"{}: {}\".format(word_lookup[key], frequency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "Now we have our vectorized matrix, we can start to use machine learning models.\n",
    "\n",
    "Let's start with topic modelling.\n",
    "\n",
    "Topic modelling is essentially running a clustering algorithm over your vectorized dataset to look for natural islands.\n",
    "\n",
    "We are going to use [Latent Dirichlet Allocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html), a dimensionality reduction algorithm typically used for LDA. It will assign dimensional scores for each document, we can then use the highest scoring dimensions to cluster documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00666706 0.00666759 0.62162255 0.00666785 0.00666759 0.00666719\n",
      " 0.0066668  0.00666729 0.00666841 0.32503766]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import LDA class\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Create topic model class (10 topics)\n",
    "topic_model = LatentDirichletAllocation(n_components=10)\n",
    "\n",
    "# Fit topic model\n",
    "topic_model.fit(vectorized_lines)\n",
    "\n",
    "# Calculate topic scores\n",
    "line_topic_scores = topic_model.transform(vectorized_lines)\n",
    "\n",
    "# Print the first line, to check it has worked\n",
    "print(line_topic_scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are ten numbers, these correspond to the ten topics. The simplest way to assign a document to a topic is to assign each document the topic with the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use `line_topic_scores` to calculate a topic number (0-10) for each line, using the maximum score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 9, 9, 5, 7, 2, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topics = [list(scores).index(max(scores)) for scores in line_topic_scores]\n",
    "\n",
    "# Print first few to check\n",
    "print(topics[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can view documents for each topic to get a feel for what it might be!\n",
    "\n",
    "The easiest way to do this is with a pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Project Gutenberg eBook of The Complete Wo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This eBook is for the use of anyone anywhere i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: The Complete Works of William Shakespeare</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Author: William Shakespeare</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Release Date: January 1994 [eBook #100]\\n[Most...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27765</th>\n",
       "      <td>Professor Michael S. Hart was the originator o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27766</th>\n",
       "      <td>Project Gutenberg-tm eBooks are often created ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27767</th>\n",
       "      <td>Most people start at our Web site which has th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27768</th>\n",
       "      <td>This Web site includes information about Proje...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27769</th>\n",
       "      <td>\\n</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27770 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Line  Topic\n",
       "0      The Project Gutenberg eBook of The Complete Wo...      2\n",
       "1      This eBook is for the use of anyone anywhere i...      2\n",
       "2       Title: The Complete Works of William Shakespeare      2\n",
       "3                            Author: William Shakespeare      9\n",
       "4      Release Date: January 1994 [eBook #100]\\n[Most...      9\n",
       "...                                                  ...    ...\n",
       "27765  Professor Michael S. Hart was the originator o...      4\n",
       "27766  Project Gutenberg-tm eBooks are often created ...      3\n",
       "27767  Most people start at our Web site which has th...      2\n",
       "27768  This Web site includes information about Proje...      2\n",
       "27769                                                 \\n      9\n",
       "\n",
       "[27770 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Display function to view data (jupyter notebooks only)\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a data frame with the origional lines and their topics\n",
    "df = pd.DataFrame()\n",
    "df.loc[:, \"Line\"] = lines\n",
    "df.loc[:, \"Topic\"] = topics\n",
    "\n",
    "# Display dataframe to show\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the first few documents in topic 6 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TWELFTH NIGHT; OR, WHAT YOU WILL</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>LAFEW.\\nHow called you the man you speak of, m...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>LAFEW.\\nI would it were not notorious. Was thi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>HELENA.\\nI do affect a sorrow indeed, but I ha...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>LAFEW.\\nHow understand we that?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27097</th>\n",
       "      <td>PAULINA.\\nI am sorry, sir, I have thus far sti...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27103</th>\n",
       "      <td>LEONTES.\\nWhat you can make her do\\nI am conte...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27104</th>\n",
       "      <td>PAULINA.\\nIt is requirâd\\nYou do awake your ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27114</th>\n",
       "      <td>[_Presenting Perdita who kneels to Hermione._]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27172</th>\n",
       "      <td>I.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4784 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Line  Topic\n",
       "48                      TWELFTH NIGHT; OR, WHAT YOU WILL      6\n",
       "391    LAFEW.\\nHow called you the man you speak of, m...      6\n",
       "397    LAFEW.\\nI would it were not notorious. Was thi...      6\n",
       "401    HELENA.\\nI do affect a sorrow indeed, but I ha...      6\n",
       "405                      LAFEW.\\nHow understand we that?      6\n",
       "...                                                  ...    ...\n",
       "27097  PAULINA.\\nI am sorry, sir, I have thus far sti...      6\n",
       "27103  LEONTES.\\nWhat you can make her do\\nI am conte...      6\n",
       "27104  PAULINA.\\nIt is requirâd\\nYou do awake your ...      6\n",
       "27114     [_Presenting Perdita who kneels to Hermione._]      6\n",
       "27172                                                 I.      6\n",
       "\n",
       "[4784 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(df[df[\"Topic\"] == 6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this isn't always hugely useful, we can also look at the top words used per topic using the `topic_model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10000807 1.74640525 0.10000371 ... 0.10011028 0.1        0.10001298]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_components = topic_model.components_\n",
    "\n",
    "# Show components for topic 6\n",
    "print(topic_components[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This components list has an index for every word, we need to calculate a sorted list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34817, 15517, 20611, 30902, 34837, 16498, 33911, 14395, 9389, 16453]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# I Googled this one, a trick of the trade is being proficient at Google\n",
    "\n",
    "# It's good practice to always include a link when you do use code you find online for future reference\n",
    "# https://stackoverflow.com/questions/7851077/how-to-return-index-of-a-sorted-list\n",
    "\n",
    "my_list = topic_components[6]\n",
    "sorted_indicies = sorted(range(len(my_list)), key=lambda k: my_list[k], reverse=True)\n",
    "\n",
    "# Print top ten indicies\n",
    "print(sorted_indicies[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then map these top indicies back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'i', 'not', 'to', 'your', 'it', 'what', 'have', 'do', 'is']\n"
     ]
    }
   ],
   "source": [
    "print([word_lookup[i] for i in sorted_indicies[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! There are a lot of short words here.\n",
    "\n",
    "Words like \"you\", \"I\", \"to\" and \"it\" are considered stop words. Typically in NLP we ignore stop words.\n",
    "\n",
    "Fortunately we can get a list of English language stop words from the [NLTK](https://www.nltk.org/) package. \n",
    "\n",
    "Below I re-run the steps to this point with stop words excluded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'fair', 'like', 'yet', 'hath', 'doth', 'eyes', 'night', 'shall', 'make']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Code to download stopwords if required\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Import stopword list\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Create a new tokenizer to remove stopwords\n",
    "def new_tokenizer(line):\n",
    "    first_stage = split_into_words(line)\n",
    "    return list(filter(lambda w: w not in english_stopwords, first_stage))\n",
    "\n",
    "# Instantiate our vectorizer (it works just like any other sklearn class)\n",
    "# We need to provide our tokenizer function as a parameter\n",
    "vectorizer = CountVectorizer(tokenizer=new_tokenizer)\n",
    "\n",
    "# Fit vectorizer\n",
    "vectorizer.fit(lines)\n",
    "\n",
    "# Vectorize (transform) our data\n",
    "vectorized_lines = vectorizer.transform(lines)\n",
    "\n",
    "# Create topic model class (10 topics)\n",
    "topic_model = LatentDirichletAllocation(n_components=10)\n",
    "\n",
    "# Fit topic model\n",
    "topic_model.fit(vectorized_lines)\n",
    "\n",
    "# Calculate topic scores\n",
    "line_topic_scores = topic_model.transform(vectorized_lines)\n",
    "\n",
    "# Topic components\n",
    "topic_components = topic_model.components_\n",
    "\n",
    "# Create a word lookup dictionary.\n",
    "# The vocabulary_ dictionary from the vectorizer is the wrong way round\n",
    "word_lookup = {value: key for key, value in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# A function to pull out top words for each topic\n",
    "def top_words(components):\n",
    "    sorted_indicies = sorted(range(len(components)), key=lambda k: components[k], reverse=True)\n",
    "    return [word_lookup[i] for i in sorted_indicies[:10]]\n",
    "\n",
    "# Print top words for topic 6\n",
    "print(top_words(topic_components[6]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['scene', 'lord', 'hamlet', 'act', 'ii', 'iago', 'good', 'room', 'house', 'iii']\n",
      "Topic 1: ['shall', 'thou', 'thy', 'let', 'us', 'hath', 'make', 'would', 'upon', 'may']\n",
      "Topic 2: ['shall', 'come', 'must', 'palamon', 'like', 'mrs', 'arcite', 'see', 'let', 'othello']\n",
      "Topic 3: ['love', 'would', 'valentine', 'hath', 'upon', 'proteus', 'sweet', 'shall', 'time', 'much']\n",
      "Topic 4: ['sir', 'good', 'well', 'lord', '', 'come', 'would', 'master', 'duke', 'man']\n",
      "Topic 5: ['', '_exit_', '_exeunt_', '_exeunt', '_exit', 'claudio', 'benedick', 'reenter', 'leonato', 'beatrice']\n",
      "Topic 6: ['would', 'fair', 'like', 'yet', 'hath', 'doth', 'eyes', 'night', 'shall', 'make']\n",
      "Topic 7: ['enter', '', 'antony', 'caesar', 'king', 'cleopatra', 'brutus', 'lord', 'duke', 'two']\n",
      "Topic 8: ['thou', 'thee', 'thy', 'art', 'iâll', 'hast', 'man', 'dost', 'romeo', 'come']\n",
      "Topic 9: ['king', 'thy', 'thou', 'thee', 'queen', 'lord', 'shall', 'gloucester', 'henry', 'richard']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Top words for all topics\n",
    "for topic in range(10):\n",
    "    print(\"Topic {}: {}\".format(topic, top_words(topic_components[topic])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling\n",
    "\n",
    "Not only can we do unsupervised learning for NLP, but also supervised. For example, let's train a model to differentiate between lines from Shakespeare's complete works and [War and Peace](https://www.gutenberg.org/ebooks/2600)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThe Project Gutenberg EBook of War and Peace, by Leo Tolstoy', 'This eBook is for the use of anyone anywhere at no cost and with almost\\nno restrictions whatsoever. You may copy it, give it away or re-use\\nit under the terms of the Project Gutenberg License included with this\\neBook or online at www.gutenberg.org', '\\nTitle: War and Peace', 'Author: Leo Tolstoy', 'Translators: Louise and Aylmer Maude']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download War and Peace\n",
    "response_wp = requests.get(\"https://www.gutenberg.org/files/2600/2600-0.txt\")\n",
    "raw_text_wp = response_wp.text\n",
    "\n",
    "# Trim first three characters (formatting)\n",
    "trim_text_wp = raw_text_wp[3:]\n",
    "\n",
    "# Split up into individual lines\n",
    "# In this dataset, individual lines are split by two new line characters\n",
    "lines_wp = trim_text_wp.replace(\"\\r\", \"\").split(\"\\n\\n\")\n",
    "\n",
    "# Print the first few lines to check this has worked\n",
    "print(lines_wp[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train and assess a predictive model, we must be able to train and test on two different sets, containing samples from both Shakespeare and Tolstoy. Producing a Pandas data frame will make this easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lines</th>\n",
       "      <th>Shakespeare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Project Gutenberg eBook of The Complete Wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This eBook is for the use of anyone anywhere i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: The Complete Works of William Shakespeare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Author: William Shakespeare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Release Date: January 1994 [eBook #100]\\n[Most...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40715</th>\n",
       "      <td>\\nMost people start at our Web site which has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40716</th>\n",
       "      <td>http://www.gutenberg.org</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40717</th>\n",
       "      <td>This Web site includes information about Proje...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40718</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40719</th>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lines  Shakespeare\n",
       "0      The Project Gutenberg eBook of The Complete Wo...            1\n",
       "1      This eBook is for the use of anyone anywhere i...            1\n",
       "2       Title: The Complete Works of William Shakespeare            1\n",
       "3                            Author: William Shakespeare            1\n",
       "4      Release Date: January 1994 [eBook #100]\\n[Most...            1\n",
       "...                                                  ...          ...\n",
       "40715  \\nMost people start at our Web site which has ...            0\n",
       "40716                           http://www.gutenberg.org            0\n",
       "40717  This Web site includes information about Proje...            0\n",
       "40718                                                               0\n",
       "40719                                                 \\n            0\n",
       "\n",
       "[40720 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Produce Shakespeare data frame\n",
    "shakespeare_df = pd.DataFrame()\n",
    "shakespeare_df.loc[:, \"Lines\"] = lines\n",
    "shakespeare_df[\"Shakespeare\"] = 1\n",
    "\n",
    "# Produce War and Peace data frame\n",
    "wp_df = pd.DataFrame()\n",
    "wp_df.loc[:, \"Lines\"] = lines_wp\n",
    "wp_df[\"Shakespeare\"] = 0\n",
    "\n",
    "# Combine the two data frames\n",
    "combined_df = shakespeare_df.append(wp_df, ignore_index=True)\n",
    "\n",
    "# Display to check it has worked\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now we can train a machine learning model. [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) models are commonly used for NLP, so we will use one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import sklearn train test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into a test and training set\n",
    "train, test = train_test_split(combined_df, test_size=0.2)\n",
    "\n",
    "# Train vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=new_tokenizer)\n",
    "vectorizer.fit(train[\"Lines\"])\n",
    "\n",
    "# Vectorize our training set\n",
    "train_vectorized = vectorizer.transform(train[\"Lines\"])\n",
    "\n",
    "# Use this to train a machine learning model\n",
    "\n",
    "# Import a Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train model\n",
    "predictive_model = GaussianNB()\n",
    "predictive_model.fit(train_vectorized.toarray(), train[\"Shakespeare\"])\n",
    "\n",
    "# Now we can test!\n",
    "\n",
    "# Vectorize test set\n",
    "test_vectorized = vectorizer.transform(test[\"Lines\"])\n",
    "\n",
    "# Predict\n",
    "predictions = predictive_model.predict(test_vectorized.toarray())\n",
    "\n",
    "# Print first few predictions to check they are as expected\n",
    "print(predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate percentage accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9209233791748527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(test[\"Shakespeare\"], predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very encouraging! We would expect Shakespeare and Tolstoy to be easy to tell apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Write your own code to test user entered strings as either Shakespeare or Tolstoy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beware the ides of March\n",
      "Shakespeare!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to test a line\n",
    "def shakespeare_or_tolstoy(line):\n",
    "    vectorized = vectorizer.transform([line])\n",
    "    pred = predictive_model.predict(vectorized.toarray())[0]\n",
    "    if pred == 1:\n",
    "        print(\"Shakespeare!\")\n",
    "    else:\n",
    "        print(\"Tolstoy!\")\n",
    "\n",
    "# Take an input\n",
    "my_input = input()\n",
    "\n",
    "# Test input\n",
    "shakespeare_or_tolstoy(my_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "We have covered:\n",
    "* Tokenization\n",
    "* Stop words\n",
    "* Vectorization\n",
    "* Topic Modelling\n",
    "* Predictive Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Write your own predictive model to differentiate between two open datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
